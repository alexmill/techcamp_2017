{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://github.com/alexmill/techcamp_2017/raw/master/session7/movies.json\")\n",
    "all_movies = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = all_movies[0:10]\n",
    "titles = [m[\"title\"] for m in movies]\n",
    "summaries = [m[\"summary\"] for m in movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Godfather',\n",
       " 'The Shawshank Redemption',\n",
       " \"Schindler's List\",\n",
       " 'Raging Bull',\n",
       " 'Casablanca',\n",
       " \"One Flew Over the Cuckoo's Nest\",\n",
       " 'Gone with the Wind',\n",
       " 'Citizen Kane',\n",
       " 'The Wizard of Oz',\n",
       " 'Titanic']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sanitize(raw_string):\n",
    "    # Removes all non-alphanumeric characters\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', raw_string)\n",
    "    out = \" \".join(s.split()).lower()\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in late summer 1945 guests are gathered for the wedding reception of don vito corleone s daughter co'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = sanitize(summaries[0])\n",
    "cleaned[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 176), ('to', 98), ('and', 96), ('a', 69), ('s', 64), ('in', 60), ('is', 59), ('his', 59), ('he', 58), ('michael', 56)]\n"
     ]
    }
   ],
   "source": [
    "tokens = get_tokens(cleaned)\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important!\n",
    "\n",
    "Before you proceed, make sure you download the NLTK data that contains English-language stopwords. To do so, run the following commands after import nltk into your Python session:\n",
    "\n",
    "```python\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import porter\n",
    "stemmer = porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed = [stemmer.stem(t) for t in tokens]\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed = stem_tokens(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('michael', 56), ('famili', 31), ('corleon', 30), ('sonni', 28), ('carlo', 24), ('conni', 19), ('meet', 18), ('sollozzo', 16), ('kill', 15), ('hagen', 13)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(stemmed)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All at once!\n",
    "def pre_process_docs(docs):\n",
    "    documents = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        cleaned = sanitize(doc)\n",
    "        tokens = get_tokens(cleaned)\n",
    "        filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "        stemmed = stem_tokens(filtered)\n",
    "        processed = \" \".join(stemmed)\n",
    "        documents.append(processed)\n",
    "        print(\"{: >3} {: <35}: {: <5} tokens\".format(i, titles[i], len(stemmed)))\n",
    "    return(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 The Godfather                      : 1686  tokens\n",
      "  1 The Shawshank Redemption           : 1322  tokens\n",
      "  2 Schindler's List                   : 661   tokens\n",
      "  3 Raging Bull                        : 477   tokens\n",
      "  4 Casablanca                         : 490   tokens\n",
      "  5 One Flew Over the Cuckoo's Nest    : 625   tokens\n",
      "  6 Gone with the Wind                 : 2225  tokens\n",
      "  7 Citizen Kane                       : 737   tokens\n",
      "  8 The Wizard of Oz                   : 588   tokens\n",
      "  9 Titanic                            : 407   tokens\n"
     ]
    }
   ],
   "source": [
    "# Construct set of sanitized/tokenized/stemmed documents\n",
    "# This might take a while!\n",
    "documents = pre_process_docs(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important!\n",
    "\n",
    "Make sure you install sklearn. (It comes bundled with Anaconda, so no need to re-install if you installed the full distribution.)\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "\n",
    "# Term Frequency\n",
    "\n",
    "Term frequency simply refers to the counts of each word in a given document. In NLP, it is common to represent every document as a vector, with every entry in the vector corresponding to a word in the the vocabulary of your corpus.\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "We sometimes want to characterize documents by the words that are *most* distinctive to those documents, among all the documents in our corpus. Note that any given term is more likely to appear the longer a document is. So sometimes, mere term frequency doesn't tell us everything we need to know about the relationship between a term and its document. In these situations, we use \"term frequency, inverse document frequency\" or TF-IDF.\n",
    "\n",
    "$$ \\text{score} = \\left( \\text{# occurrences of } term \\text{ in } doc \\right) \\times \\log \\frac{\\text{# documents}}{\\text{# documents containing } term} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2762)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of your data matrix\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# = (number of documents, number of unique words in your corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '100', '14', '15', '16', '17', '1861', '1862', '1863', '1865', '1866', '1871', '1900', '1912', '1929', '1939', '1940', '1941', '1945', '1947']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the raw matrix itself.\n",
    "# But don't print the whole thing!!!\n",
    "num_rows = 5\n",
    "num_words = 20\n",
    "words = tf_feature_names[0:num_words]\n",
    "print(words)\n",
    "tf[0:num_rows, 1:num_words].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_top(vectorizer, tfidf_result, top_n=10):\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for item in sorted_scores[:top_n]:\n",
    "        print(\"{0:20}: {1}\".format(item[0], round(item[1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scarlett            : 113\n",
      "andi                : 75\n",
      "rhett               : 67\n",
      "michael             : 57\n",
      "tell                : 56\n",
      "ashley              : 42\n",
      "kane                : 42\n",
      "melani              : 41\n",
      "dorothi             : 40\n",
      "red                 : 39\n"
     ]
    }
   ],
   "source": [
    "display_top(tf_vectorizer, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andi                : 0.79\n",
      "kane                : 0.78\n",
      "scarlett            : 0.72\n",
      "rick                : 0.7\n",
      "dorothi             : 0.69\n",
      "schindler           : 0.63\n",
      "mcmurphi            : 0.58\n",
      "rose                : 0.56\n",
      "jake                : 0.54\n",
      "michael             : 0.53\n"
     ]
    }
   ],
   "source": [
    "display_top(tfidf_vectorizer, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "There are many ways to measure \"similarity\" among word count vectors. A simple and common one is called **cosine similarity**. If $\\mathbf{A}$ and $\\mathbf{B}$ are two n-dimensional vectors, their cosine simialirty is defined as:\n",
    "\n",
    "$$ \\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} } $$\n",
    "\n",
    "One could also think of similarity as the opposite of distance. In this case, we could take a simple distance function, e.g., $n$-dimensional Euclidean distance, and define similarity as it's negative or inverse.\n",
    "\n",
    "$$ \\text{similarity} = - \\|\\mathbf{A - B}\\| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate similarity between documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_mat = cosine_similarity(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_most_similar(movie_title, n=2, similarity_matrix=sim_mat, movie_names=titles):\n",
    "    focal_index = movie_names.index(movie_title)\n",
    "    row = similarity_matrix[focal_index,:]\n",
    "    sorted_row = sorted([m for m in enumerate(row) if m[0]!=focal_index], key=lambda x: x[1], reverse=True)\n",
    "    print(\"Most similar movies:\")\n",
    "    for i in range(n):\n",
    "        entry = sorted_row[i]\n",
    "        title_index = entry[0]\n",
    "        title = movie_names[title_index]\n",
    "        print(\"{}: {}\".format(i+1,title))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar movies:\n",
      "1: Gone with the Wind\n"
     ]
    }
   ],
   "source": [
    "get_most_similar(\"The Godfather\", n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
